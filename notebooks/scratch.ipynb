{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lecture 2 - Python, Jupyter and APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 2.1 Jupyter notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip --version\n",
    "%pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flask\n",
    "import flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import site\n",
    "print(site.getsitepackages())\n",
    "!ls -l {site.getsitepackages()[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 2.3 API requests using `requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "tasks = {}\n",
    "i = 1\n",
    "\n",
    "@app.post(\"/tasks\")\n",
    "def create():\n",
    "    global i\n",
    "    if not request.json: abort(400)\n",
    "    t = {\"id\": i, \"state\": \"created\", \"data\": request.json}\n",
    "    tasks[i] = t\n",
    "    i += 1\n",
    "    return jsonify(t), 201\n",
    "\n",
    "# This starts Flask's blocking event loop in same thread as Jupyter\n",
    "# Subsequent cells can't run until Flask stops serving\n",
    "#app.run()\n",
    "\n",
    "# Inside a Jupyter notebook then, run Flask in a background process\n",
    "# `use_reloader = False` is mandatory in a Jupyter notebook\n",
    "def run():\n",
    "    app.run(host=\"127.0.0.1\", port=5000, use_reloader=False)\n",
    "threading.Thread(target=run, daemon=True).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic test using curl\n",
    "!curl -X POST http://127.0.0.1:5000/tasks -H \"Content-Type: application/json\" -d '{\"type\": \"demo\", \"params\": {\"x\": 1}}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.post(\n",
    "    \"http://127.0.0.1:5000/tasks\",\n",
    "    json={\"type\": \"demo\", \"params\": {\"x\": 1}}\n",
    ")\n",
    "print(r.status_code, r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify, abort, request\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "tasks = {}\n",
    "\n",
    "i = 1\n",
    "\n",
    "@app.post(\"/tasks\")\n",
    "def create():\n",
    "    global i\n",
    "    if not request.json: abort(400)\n",
    "    t = {\"id\": i, \"state\": \"created\", \"data\": request.json}\n",
    "    tasks[i] = t\n",
    "    i += 1\n",
    "    return jsonify(t), 201\n",
    "\n",
    "@app.get(\"/tasks\")\n",
    "def list_tasks():\n",
    "    return jsonify(list(tasks.values()))\n",
    "\n",
    "@app.get(\"/tasks/<int:i>\")\n",
    "def get_task(i):\n",
    "    return jsonify(tasks[i]) if i in tasks else abort(404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    app.run(host=\"127.0.0.1\", port=5000, use_reloader=False)\n",
    "threading.Thread(target=run, daemon=True).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get(\"http://127.0.0.1:5000/tasks\")\n",
    "print(r.status_code, r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.post(\n",
    "    \"http://127.0.0.1:5000/tasks\",\n",
    "    json={\"type\": \"demo\", \"params\": {\"x\": 1}}\n",
    ")\n",
    "print(r.status_code, r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(\n",
    "    \"http://127.0.0.1:5000/tasks\",\n",
    "    json={\"type\": \"demo\", \"params\": {\"x\": 1}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(\n",
    "    \"http://127.0.0.1:5000/tasks\",\n",
    "    json={\"type\": \"demo\", \"params\": {\"x\": 1}}\n",
    ").status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(\"http://127.0.0.1:5000/tasks\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(\"http://127.0.0.1:5000/tasks/2\").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# Lecture 3 - Visualising Fields and Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 3.2 ecCodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eccodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!grib_ls -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eccodes.codes_get_api_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Read GRIB2 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find .. -name \"*.grib2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "grib_file = \"../e-ai_ml2/course/code/code03/ifs_2t.grib2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(grib_file, \"rb\") as f:\n",
    "    while True:\n",
    "        gid = eccodes.codes_grib_new_from_file(f)\n",
    "        if gid is None: break\n",
    "\n",
    "        short = eccodes.codes_get(gid, \"shortName\")\n",
    "        level = eccodes.codes_get(gid, \"level\")\n",
    "        size  = eccodes.codes_get_size(gid, \"values\")\n",
    "\n",
    "        print(short, level, size)\n",
    "\n",
    "        eccodes.codes_release(gid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Download GRIB2 file from ECMWF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecmwf.opendata import Client\n",
    "\n",
    "client = Client(\n",
    "    source = \"ecmwf\",\n",
    "    model = \"ifs\",\n",
    ")\n",
    "\n",
    "client.retrieve(\n",
    "    time = 0,\n",
    "    type = \"fc\",\n",
    "    step = 24,\n",
    "    param = [\"2t\", \"msl\"],\n",
    "    target = \"ifs_2t.grib2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *.grib2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Download from DWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "base_url = \"http://opendata.dwd.de/weather/nwp/icon/grib/00/t_2m/\"\n",
    "now = datetime.datetime.now(datetime.UTC)\n",
    "filename = f\"icon_global_icosahedral_single-level_{now:%Y%m%d}00_000_T_2M.grib2.bz2\"\n",
    "url = base_url + filename\n",
    "grib_filename = filename[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "wget.download(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "\n",
    "with bz2.open(filename, \"rb\") as f_in, open(grib_filename, \"wb\") as f_out:\n",
    "    f_out.write(f_in.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *.grib2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eccodes\n",
    "with open(grib_filename, \"rb\") as f:\n",
    "    while True:\n",
    "        gid = eccodes.codes_grib_new_from_file(f)\n",
    "        if gid is None: break\n",
    "\n",
    "        short = eccodes.codes_get(gid, \"shortName\")\n",
    "        level = eccodes.codes_get(gid, \"level\")\n",
    "        size  = eccodes.codes_get_size(gid, \"values\")\n",
    "\n",
    "        print(short, level, size)\n",
    "\n",
    "        eccodes.codes_release(gid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Extract and list metadata keys from a GRIB file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eccodes\n",
    "with open(grib_filename, \"rb\") as f:\n",
    "    while True:\n",
    "        gid = eccodes.codes_grib_new_from_file(f)\n",
    "        if gid is None: break\n",
    "\n",
    "        key_iterator = eccodes.codes_keys_iterator_new(gid)\n",
    "        keys = []\n",
    "\n",
    "        while eccodes.codes_keys_iterator_next(key_iterator):\n",
    "            keyname = eccodes.codes_keys_iterator_get_name(key_iterator)\n",
    "            if keyname not in ['section2Padding', 'codedValues', 'values']:\n",
    "                value = eccodes.codes_get_string(gid, keyname)\n",
    "            keys.append((keyname, value))\n",
    "\n",
    "        eccodes.codes_release(gid)\n",
    "\n",
    "        for key, value in keys:\n",
    "              print(f\"Key: {key:40} Value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eccodes\n",
    "with open(grib_file, \"rb\") as f:\n",
    "    while True:\n",
    "        gid = eccodes.codes_grib_new_from_file(f)\n",
    "        if gid is None: break\n",
    "\n",
    "        key_iterator = eccodes.codes_keys_iterator_new(gid)\n",
    "        keys = []\n",
    "\n",
    "        while eccodes.codes_keys_iterator_next(key_iterator):\n",
    "            keyname = eccodes.codes_keys_iterator_get_name(key_iterator)\n",
    "            if keyname not in ['section2Padding', 'codedValues', 'values']:\n",
    "                value = eccodes.codes_get_string(gid, keyname)\n",
    "            keys.append((keyname, value))\n",
    "\n",
    "        eccodes.codes_release(gid)\n",
    "\n",
    "        for key, value in keys:\n",
    "              print(f\"Key: {key:40} Value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install cartopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(grib_file, \"rb\") as f:\n",
    "    # First message is pressure\n",
    "    gid = eccodes.codes_grib_new_from_file(f)\n",
    "\n",
    "nx = eccodes.codes_get(gid, \"Ni\")\n",
    "ny = eccodes.codes_get(gid, \"Nj\")\n",
    "values = eccodes.codes_get_array(gid, \"values\")\n",
    "#field = values.reshape(ny, nx)\n",
    "field = values.reshape(ny, nx) / 100.0  # Pa → hPa\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 3.5))\n",
    "#plt.imshow(field)\n",
    "im = plt.imshow(field)\n",
    "plt.title(\"IFS Mean Sea Level Pressure (hPa)\")\n",
    "plt.colorbar(im, label=\"Pressure (hPa)\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "import os\n",
    "out_dir = \"../assets/images\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, \"grib_plot_with_eccodes_ifs_pressure.png\")\n",
    "plt.savefig(out_path, dpi=300, bbox_inches=\"tight\", pad_inches=0.1)\n",
    "!ls -ltr {out_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(grib_file, \"rb\") as f:\n",
    "    # Run twice to get the second message (T2m)\n",
    "    gid = eccodes.codes_grib_new_from_file(f)\n",
    "    gid = eccodes.codes_grib_new_from_file(f)\n",
    "\n",
    "nx = eccodes.codes_get(gid, \"Ni\")\n",
    "ny = eccodes.codes_get(gid, \"Nj\")\n",
    "values = eccodes.codes_get_array(gid, \"values\")\n",
    "field = values.reshape(ny, nx)\n",
    "\n",
    "plt.figure(figsize=(7, 3.5))\n",
    "#plt.imshow(field)\n",
    "im = plt.imshow(field)\n",
    "plt.title(\"IFS 2m Temperature (K)\")\n",
    "plt.colorbar(im, label=\"K\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "#plt.show()\n",
    "\n",
    "import os\n",
    "out_dir = \"../assets/images\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, \"grib_plot_with_eccodes_ifs_t2m.png\")\n",
    "plt.savefig(out_path, dpi=300, bbox_inches=\"tight\", pad_inches=0.1)\n",
    "!ls -ltr {out_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5), subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "ax.coastlines()\n",
    "ax.add_feature(cfeature.BORDERS)\n",
    "\n",
    "lats   = eccodes.codes_get_array(gid, \"latitudes\")\n",
    "lons   = eccodes.codes_get_array(gid, \"longitudes\")\n",
    "lat   = lats.reshape(ny, nx)\n",
    "lon   = lons.reshape(ny, nx)\n",
    "\n",
    "ax.pcolormesh(lon, lat, field, transform=ccrs.PlateCarree(), cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install scipy\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "\n",
    "def load_grib(file, var):\n",
    "    \"\"\"Loads specified variable from GRIB file.\"\"\"\n",
    "    with open(file, 'rb') as f:\n",
    "        while (gid := eccodes.codes_grib_new_from_file(f)) is not None:\n",
    "            if eccodes.codes_get(gid, \"shortName\") == var:\n",
    "                vals = eccodes.codes_get_array(gid, \"values\")\n",
    "                eccodes.codes_release(gid)\n",
    "                return vals\n",
    "            eccodes.codes_release(gid)\n",
    "    return None\n",
    "\n",
    "def interpolate_to_grid(lat, lon, t2m, bbox, grid_res=0.25):\n",
    "    \"\"\"Interpolates T2M data onto a regular lat/lon grid.\"\"\"\n",
    "    latmin, latmax, lonmin, lonmax = bbox\n",
    "\n",
    "    # Define a smooth regular grid\n",
    "    grid_lat = np.arange(latmin, latmax, grid_res)\n",
    "    grid_lon = np.arange(lonmin, lonmax, grid_res)\n",
    "    lon_grid, lat_grid = np.meshgrid(grid_lon, grid_lat)\n",
    "\n",
    "    points = np.column_stack((lon.ravel(), lat.ravel()))\n",
    "    values = t2m.ravel()\n",
    "    xi = np.column_stack((lon_grid.ravel(), lat_grid.ravel()))\n",
    "    t2m_grid = griddata(points, values, xi, method='cubic')\n",
    "    t2m_grid = t2m_grid.reshape(lon_grid.shape)\n",
    "    \n",
    "    return lon_grid, lat_grid, t2m_grid\n",
    "\n",
    "\n",
    "def plot_t2m_grid(lat, lon, t2m, bbox, title, fname):\n",
    "    \"\"\"Plots interpolated 2m temperature as a smooth heatmap.\"\"\"\n",
    "    lon_grid, lat_grid, t2m_grid = interpolate_to_grid(lat, lon, t2m, bbox)\n",
    "\n",
    "    # Set reasonable aspect ratio based on bounding box size\n",
    "    lon_range = bbox[3] - bbox[2]\n",
    "    lat_range = bbox[1] - bbox[0]\n",
    "    aspect_ratio = lon_range / lat_range\n",
    "    figsize = (10, max(5, 10 / aspect_ratio))  # Maintain consistent width & prevent extreme height\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.set_extent([bbox[2], bbox[3], bbox[0], bbox[1]])\n",
    "    ax.add_feature(cfeature.LAND, edgecolor='black')\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "    # Use smooth interpolation and correct aspect ratio\n",
    "    img = ax.imshow(t2m_grid, extent=[bbox[2], bbox[3], bbox[0], bbox[1]], origin='lower',\n",
    "                    cmap='jet', transform=ccrs.PlateCarree(), aspect='auto', interpolation='bicubic')\n",
    "\n",
    "    plt.colorbar(img, label=\"Temperature (K)\")\n",
    "    plt.title(title)\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches='tight')  # Reduce DPI for smaller file size\n",
    "    #plt.show()\n",
    "\n",
    "import os\n",
    "out_dir = \"../assets/images\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "lat = load_grib(\"../e-ai_ml2/course/code/code03/icon_lat.grib\", \"tlat\")\n",
    "lon = load_grib(\"../e-ai_ml2/course/code/code03/icon_lon.grib\", \"tlon\")\n",
    "t2m = load_grib(\"../e-ai_ml2/course/code/code03/icon_t2m.grib\", \"2t\")\n",
    "\n",
    "# Plot interpolated global and Germany views\n",
    "out_path = os.path.join(out_dir, \"grib_plot_with_eccodes_icon_t2m_global_interp.png\")\n",
    "plot_t2m_grid(lat, lon, t2m, (-90, 90, -180, 180), \"ICON Interpolated Global 2m Temperature\", \"icon_t2m_global_interp.png\")\n",
    "out_path = os.path.join(out_dir, \"grib_plot_with_eccodes_icon_t2m_germany_interp.png\")\n",
    "plot_t2m_grid(lat, lon, t2m, (47, 55, 5, 15), \"ICON Interpolated 2m Temperature over Germany\", \"icon_t2m_germany_interp.png\")\n",
    "\n",
    "!ls -ltr {out_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## 3.3 Accessing SYNOP observation files from NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find ../e-ai_ml2 -name \"*.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install netCDF4\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "filename = \"../e-ai_ml2/course/code/code03/synop.nc\"\n",
    "\n",
    "ncfile = Dataset(filename, \"r\")\n",
    "\n",
    "lats = ncfile.variables[\"MLAH\"][:]\n",
    "lons = ncfile.variables[\"MLOH\"][:]\n",
    "temps = ncfile.variables[\"MTDBT\"][:]\n",
    "\n",
    "lats = np.array(lats)\n",
    "lons = np.array(lons)\n",
    "temps = np.array(temps)\n",
    "\n",
    "ncfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=1e+20\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "#projections = [[ccrs.PlateCarree(), \"PlateCarree\"]]\n",
    "projections=[[ccrs.PlateCarree(), \"PlateCarree\"], \n",
    "                                  [ccrs.TransverseMercator(), \"TransverseMercator\"],\n",
    "                                  [ccrs.Mercator(), \"Mercator\"],\n",
    "                                  [ccrs.EuroPP(), \"EuroPP\"],\n",
    "                                  [ccrs.Geostationary(), \"Geostationary\"],\n",
    "                                  [ccrs.Stereographic(), \"Stereographic\"]]\n",
    "# Filter out large missing values\n",
    "valid_mask = (temps < threshold) & np.isfinite(temps)\n",
    "lats, lons, temps = lats[valid_mask], lons[valid_mask], temps[valid_mask]\n",
    "\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "for projection in projections:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6), subplot_kw={'projection': projection[0]})\n",
    "        scatter = ax.scatter(lons, lats, c=temps, cmap='jet', s=5, alpha=0.7, transform=ccrs.PlateCarree())\n",
    "\n",
    "        # Add map features\n",
    "        ax.coastlines()\n",
    "        ax.add_feature(cfeature.BORDERS, edgecolor='gray')\n",
    "        ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
    "\n",
    "        # Add colorbar with better spacing\n",
    "        cbar = plt.colorbar(scatter, ax=ax, fraction=0.04, pad=0.08)  \n",
    "        cbar.set_label(\"Temperature (K)\")\n",
    "\n",
    "        # Set title\n",
    "        plt.title(\"Temperature Observations on Map in Projection \" + projection[1])\n",
    "\n",
    "        # Save and show the plot\n",
    "        #plt.show()\n",
    "        import os\n",
    "        out_dir = \"../assets/images\"\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        out_path = os.path.join(out_dir, f\"synop_temp_{projection[1]}.png\")\n",
    "        plt.savefig(out_path, dpi=300, bbox_inches=\"tight\", pad_inches=0.1)\n",
    "        !ls -ltr {out_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "## 3.4 AIREP feedback files in NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "airep_file = \"../e-ai_ml2/course/code/code03/monAIREP.nc\"\n",
    "\n",
    "ncfile = Dataset(airep_file, \"r\")\n",
    "\n",
    "nc = 1\n",
    "for varname in ncfile.variables.keys():\n",
    "    var = ncfile.variables[varname]\n",
    "    description = getattr(var, \"longname\", \"N/A\")\n",
    "    dims = [len(ncfile.dimensions[dim]) for dim in var.dimensions]\n",
    "    shape1 = dims[0] if len (dims) > 0 else \"\"\n",
    "    shape2 = dims[1] if len (dims) > 1 else \"\"\n",
    "    print (\"{:<4} {:40} {:>10} {:>10} {:30}\".format(nc, varname, shape1, shape2, description))\n",
    "    if nc % 10 == 0:\n",
    "        print(\"-\" * 110)\n",
    "    nc += 1\n",
    "\n",
    "ncfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read header-level variables\n",
    "ncfile = Dataset(airep_file, \"r\")\n",
    "lat = ncfile.variables[\"lat\"][:]\n",
    "lon = ncfile.variables[\"lon\"][:]\n",
    "\n",
    "# Body-level variables\n",
    "varno_all = ncfile.variables[\"varno\"][:]\n",
    "obs_all = ncfile.variables[\"obs\"][:]\n",
    "l_body = ncfile.variables[\"l_body\"][:]\n",
    "\n",
    "# Expand lat/lon to match body-level observations\n",
    "ni = len(l_body)\n",
    "ie = np.repeat(range(0, ni), l_body)  # Map each body entry to its header index\n",
    "\n",
    "# varno == 2 is upper air temperature\n",
    "idx = np.where(varno_all == 2)[0]\n",
    "\n",
    "# Filter lat, lon, obs\n",
    "lat_filtered = lat[ie[idx]]\n",
    "lon_filtered = lon[ie[idx]]\n",
    "obs_filtered = obs_all[idx]\n",
    "\n",
    "var = \"level\"\n",
    "var_data = ncfile.variables[var][:]\n",
    "\n",
    "print(var_data.shape[0], len(varno_all))\n",
    "\n",
    "extra_data = var_data[idx]\n",
    "lats, lons, obs = lat_filtered, lon_filtered, obs_filtered\n",
    "heights = extra_data\n",
    "\n",
    "threshold=1e+20\n",
    "\n",
    "print(len(lats), \"Latitudes:\", lats[:5])\n",
    "print(len(lons), \"Longitudes:\", lons[:5])\n",
    "print(len(obs), \"Observations:\", obs[:5])\n",
    "if heights is not None:\n",
    "    print(len(heights), \"Heights:\", heights[:5])\n",
    "\n",
    "valid_mask = (obs < threshold) & np.isfinite(obs)\n",
    "lats, lons, obs = lats[valid_mask], lons[valid_mask], obs[valid_mask]\n",
    "\n",
    "# Keep only temperatures between -30°C and 40°C (243.15K to 313.15K)\n",
    "temp_min, temp_max = 180, 320\n",
    "physical_mask = (obs >= temp_min) & (obs <= temp_max)\n",
    "\n",
    "lats_filtered, lons_filtered, obs_filtered = lats[physical_mask], lons[physical_mask], obs[physical_mask]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "scatter = ax.scatter(lons_filtered, lats_filtered, c=obs_filtered, cmap='jet', s=2, alpha=0.7, transform=ccrs.PlateCarree())\n",
    "\n",
    "ax.coastlines()\n",
    "ax.add_feature(cfeature.BORDERS, edgecolor='gray')\n",
    "ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
    "\n",
    "# Ensure the colorbar does not exceed figure height\n",
    "cbar = fig.colorbar(scatter, ax=ax, orientation='vertical', fraction=0.04, pad=0.08, shrink=0.8)\n",
    "cbar.set_label(\"Temperature (K)\")\n",
    "\n",
    "plt.title(\"AIREP Observations\")\n",
    "#plt.show()\n",
    "import os\n",
    "out_dir = \"../assets/images\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, f\"airep.png\")\n",
    "plt.savefig(out_path, dpi=300, bbox_inches=\"tight\", pad_inches=0.1)\n",
    "!ls -ltr {out_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "## GPU access in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "d = torch.device(\"mps\")\n",
    "\n",
    "x = torch.rand((4000, 4000),device=d)\n",
    "\n",
    "t0 = time.time()\n",
    "y = torch.matmul(x, x)\n",
    "torch.mps.synchronize()\n",
    "print(\"Time = \", round(time.time()-t0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30000\n",
    "x0 = torch.rand((n, n), device=\"cpu\")\n",
    "x1 = torch.rand((n, n), device=\"cpu\")\n",
    "t0 = time.time()\n",
    "y0 = torch.matmul(x0, x0)\n",
    "y1 = torch.matmul(x1, x1)\n",
    "print(\"Time = \", round(time.time() - t0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = torch.device(\"mps:0\")\n",
    "d1 = torch.device(\"mps:1\")\n",
    "x0 = torch.rand((n, n), device=d0)\n",
    "x1 = torch.rand((n, n), device=d1)\n",
    "\n",
    "t0 = time.time()\n",
    "y0 = torch.matmul(x0, x0)\n",
    "y1 = torch.matmul(x1, x1)\n",
    "torch.mps.synchronize()\n",
    "\n",
    "print(\"Time = \", round(time.time() - t0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1, y0, y1, x, y = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "A0 = torch.rand((n//2,n), device=d0)\n",
    "A1 = torch.rand((n//2,n), device=d1)\n",
    "\n",
    "B = torch.rand((n,n), device=d0)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "C0 = A0 @ B\n",
    "C1 = A1 @ B.to(d1)\n",
    "\n",
    "torch.mps.synchronize()\n",
    "\n",
    "print(\"Time = \", round(time.time() - t0, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "### Mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.device(\"mps\")\n",
    "\n",
    "def doit(d):\n",
    "    x = torch.randn((20000, 1024), device=d)\n",
    "    W1 = torch.randn((1024, 4096), device=d)\n",
    "    W2 = torch.randn((4096, 1024), device=d)\n",
    "    t0 = time.time()\n",
    "    y = torch.nn.functional.gelu(x @ W1)\n",
    "    z = y @ W2\n",
    "    torch.mps.synchronize()\n",
    "    return round(time.time() - t0, 3)\n",
    "\n",
    "for dt in [torch.float32, torch.float16]:\n",
    "    torch.set_default_dtype(dt)\n",
    "    print(f\"{dt} time: {doit(d)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "# Lecture 4 - AI and ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "### Torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor([2., 3.], requires_grad=True)\n",
    "y = x[0]**2 + x[1]**2\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# nn.Module is the base class for models and layers\n",
    "# Holds parameters (weights and biases)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Layer 1: 1 -> 16\n",
    "        self.fc1 = nn.Linear(1,16)\n",
    "        \n",
    "        # Non-linear activation function (ReLU in this case)\n",
    "        self.relu = nn.reLU()\n",
    "        \n",
    "        # Layer 2: 16 -> 1\n",
    "        self.fc2 = nn.Linear(16,1)\n",
    "\n",
    "    # Calling `model(x)` runs the model's `forward()` method\n",
    "    # Forward pass computes predictions from inputs (x)\n",
    "    # Builds the autograd graph (if grads enables on x)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "Learning a sine function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input values x\n",
    "x = np.linspace(0, 2*np.pi, 1000)\n",
    "\n",
    "# Compute labels y = sin(x)\n",
    "y = np.sin(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset construction\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "x_t = torch.tensor(x).float().unsqueeze(1)\n",
    "y_t = torch.tensor(y).float().unsqueeze(1)\n",
    "\n",
    "data = TensorDataset(x_t, y_t)\n",
    "loader = DataLoader(data,\n",
    "                    batch_size=32,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and training loop\n",
    "\n",
    "# Learn non-linear mapping x -> \\hat{y}\n",
    "# Input: scalar x\n",
    "# Output: scalar \\hat{y}\n",
    "\n",
    "# Model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1,16), nn.ReLU(),\n",
    "    nn.Linear(16,16), nn.ReLU(),\n",
    "    nn.Linear(16,1)\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Optimiser\n",
    "opt = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = 0.01\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "#     - Compare \\hat{y} and y\n",
    "#     - Minimise prediction error\n",
    "#     - Update model parameters\n",
    "for x_b, y_b in loader:\n",
    "    \n",
    "    # Zero the gradients from the previous iteration\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # Forward pass of the model to get predictions\n",
    "    y_p = model(x_b)\n",
    "\n",
    "    # Update loss given predictions y_p\n",
    "    loss = loss_fn(y_p, y_b)\n",
    "\n",
    "    # Backpropagation - compute gradients of loss wrt parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimiser - update parameters (weights and biases) in-place\n",
    "    # given the gradients\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "# Lecture 5 - Neural Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "## Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size, hidden_size1, hidden_size2, output_size = 1, 8, 6, 1\n",
    "model = FeedForwardNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Function y = y(x)\n",
    "x = np.linspace(-2, 2, 500)\n",
    "y = 1 / (1 + np.exp(-5 * x))\n",
    "\n",
    "# Convert to tensor\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# --- Define model ----\n",
    "\n",
    "# Architecture\n",
    "class DeepFFNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1, self.fc2, self.fc3 = nn.Linear(1, 8), nn.Linear(8, 6), nn.Linear(6, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc3(\n",
    "            torch.relu(\n",
    "                self.fc2(\n",
    "                    torch.relu(\n",
    "                        self.fc1(x)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "model = DeepFFNN().to(torch.float32)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimiser\n",
    "optimiser = optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(2000):\n",
    "    optimiser.zero_grad()\n",
    "    y_pred = model(x_tensor)\n",
    "    loss = criterion(y_pred, y_tensor)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    loss_history.append(loss.item())\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch+1:4d}, Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_np = model(x_tensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "axes[0].plot(x, y, label=\"True\", linewidth=2)\n",
    "axes[0].plot(x, y_pred_np, \"r--\", label=\"NN Approx.\", linewidth=2)\n",
    "axes[0].set(title=\"Function Approximation\", xlabel=\"x\", ylabel=\"f(x)\"); axes[0].legend(); axes[0].grid()\n",
    "axes[1].semilogy(loss_history, \"r\", label=\"Loss\")\n",
    "axes[1].set(title=\"Loss Curve\", xlabel=\"Epochs\", ylabel=\"MSE\"); axes[1].legend(); axes[1].grid()\n",
    "plt.savefig(\"../assets/images/deep_nn_results.png\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "## Depth vs size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "x = np.linspace(-4, 4, 100)\n",
    "y = np.sin(np.sin(np.sin(x)))\n",
    "\n",
    "x_t = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "y_t = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "N0 = 64\n",
    "N1 = 7\n",
    "\n",
    "class Shallow(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, N0),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(N0, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Deep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, N1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(N1, N1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(N1, N1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(N1, N1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(N1, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Parameter counting function -----------------------------\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=2000):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        opt.zero_grad()\n",
    "        y_pred = model(x_t)\n",
    "        loss = loss_fn(y_pred, y_t)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [Shallow(), Deep()]\n",
    "\n",
    "params = [count_params(m) for m in models]\n",
    "print(params)\n",
    "\n",
    "# Training\n",
    "losses = [train(m) for m in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    yf = [m(x_t).numpy() for m in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference result\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x, y, label=\"true\", lw=2)\n",
    "plt.plot(x, yf[0], \"--\", label=\"shallow\")\n",
    "plt.title(\"Shallow network\")\n",
    "plt.legend(); plt.grid()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(x, y, label=\"true\", lw=2)\n",
    "plt.plot(x, yf[1], \"--\", label=\"deep\")\n",
    "plt.title(\"Deep network\")\n",
    "plt.legend(); plt.grid()\n",
    "\n",
    "plt.savefig(\"../assets/images/shallow_vs_deep.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss\n",
    "\n",
    "plt.semilogy(losses[0], label=\"shallow\")\n",
    "plt.semilogy(losses[1], label=\"deep\")\n",
    "plt.legend(); plt.grid()\n",
    "plt.title(\"Training loss\")\n",
    "plt.savefig(\"../assets/images/shallow_vs_deep_loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_true = np.gradient(y, x)\n",
    "dy_s = np.gradient(yf[0].squeeze(), x)\n",
    "dy_d = np.gradient(yf[1].squeeze(), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, dy_true, label=\"true\", linewidth=2)\n",
    "plt.plot(x, dy_s, \"--\", label=\"shallow\")\n",
    "plt.plot(x, dy_d, \"--\", label=\"deep\")\n",
    "plt.title(\"Derivatives\")\n",
    "plt.legend(); plt.grid()\n",
    "plt.savefig(\"../assets/images/shallow_vs_deep_gradients.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "## Graph Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch-geometric\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN with two hidden layers\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_feats_y):\n",
    "        super().__init__()\n",
    "\n",
    "        # Graph Convolutional Layers (Message Passing)\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels[0])\n",
    "        self.conv2 = GCNConv(hidden_channels[0], hidden_channels[1])\n",
    "\n",
    "        # Fully Connected Layers (MLP Head)\n",
    "        self.fc1 = nn.Linear(hidden_channels[1], hidden_channels[0])\n",
    "        self.fc2 = nn.Linear(hidden_channels[0], num_feats_y)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        # Message Passing with GCN Layers\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv2(x, edge_index))\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 25\n",
    "xa = 10\n",
    "x_grid = torch.linspace(0, xa, nx)\n",
    "x_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph configuration - p1 and p2 are the parametric coordinates of points on the unit circle\n",
    "p1 = torch.sin(2 * torch.pi * x_grid / xa)\n",
    "p2 = torch.cos(2 * torch.pi * x_grid / xa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_grid, p1)\n",
    "plt.plot(x_grid, p2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p1, p2)\n",
    "plt.gca().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjacency matrix (chord distance between every pair)\n",
    "diff = torch.sqrt((p1.repeat(nx, 1).T - p1)**2 + (p2.repeat(nx, 1).T - p2)**2)\n",
    "diff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge index\n",
    "threshold = 0.5\n",
    "edge_index = (diff < threshold).float().nonzero(as_tuple=False).t().contiguous()\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the connectivity\n",
    "\n",
    "x = p2.numpy()  # cos(θ) - x coordinates\n",
    "y = p1.numpy()  # sin(θ) - y coordinates\n",
    "\n",
    "# Plot edges\n",
    "edge_index_np = edge_index.numpy()\n",
    "for i, j in edge_index_np. T:\n",
    "    plt.plot([x[i], x[j]], [y[i], y[j]], 'b-', alpha=0.3, linewidth=0.5)\n",
    "\n",
    "# Plot nodes\n",
    "plt.scatter(x, y, c='red', s=50, zorder=5)\n",
    "\n",
    "plt.scatter(x[0], y[0], c='blue', s=50, zorder=5)\n",
    "plt.scatter(x[1], y[1], c='green', s=50, zorder=5)\n",
    "\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.title(f'Graph connectivity (threshold = {threshold})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node features (x) and node labels (y)\n",
    "\n",
    "# x is a matrix, each row is the coordinates of one point\n",
    "# y are random binary labels (0 or 1), the target for classification\n",
    "\n",
    "data = Data(\n",
    "    x = torch.cat((p1.unsqueeze(1), p2.unsqueeze(1)), dim=1),\n",
    "    y = torch.randint(0, 2, (nx, 1)).float(),\n",
    "    edge_index = edge_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "model = GNNModel(num_features=2, hidden_channels=[8, 16], num_feats_y=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count edges\n",
    "num_edges = edge_index.shape[1]\n",
    "\n",
    "# Degree of each node\n",
    "degree_per_node = torch.bincount(edge_index[0])\n",
    "\n",
    "print(\"Num edges               = \", num_edges)\n",
    "print(\"Average degree per node = \", round(degree_per_node.float().mean().item(), 2))\n",
    "print(\"Max degree per node     = \", degree_per_node.max().item())\n",
    "\n",
    "# Print degree of first few nodes\n",
    "for i in range(min(10, nx)):  # Print up to 10 nodes\n",
    "    print(f\"Node {i} has {degree_per_node[i].item()} neighbors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torchviz\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass to generate graph visualisation\n",
    "y_pred = model(data.x, data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = make_dot(y_pred,\n",
    "               params={**dict(model.named_parameters()), 'Input features': data.x},\n",
    "               show_attrs = True,\n",
    "               show_saved = True\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot.render(\"../assets/images/gnn_graph\", format=\"png\", cleanup = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of nodes\n",
    "nx_nodes = 12\n",
    "\n",
    "# Ellipse parameters\n",
    "a, b = 10, 4\n",
    "\n",
    "# Adjacency matrix for 4-neighbour connectivity (2 left, 2 right)\n",
    "adjm = torch.zeros((nx_nodes, nx_nodes), dtype = torch.float)\n",
    "\n",
    "# Populate\n",
    "for i in range(nx_nodes):\n",
    "    adjm[i, (i-1)%nx_nodes] = 1 # Left neighbour\n",
    "    adjm[i, (i+1)%nx_nodes] = 1 # Right neighbour\n",
    "    adjm[i, (i-2)%nx_nodes] = 1 # Second left neighbour\n",
    "    adjm[i, (i+2)%nx_nodes] = 1 # Second right neighbour\n",
    "\n",
    "print(adjm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = adjm.nonzero(as_tuple=False).t().contiguous()\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viz\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges\n",
    "edges = edge_index.t().tolist()\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Generate node positions\n",
    "tau_values = np.linspace(0, 2*np.pi, nx_nodes, endpoint=False)\n",
    "x_positions = a * np.sin(tau_values)\n",
    "y_positions = b * np.cos(tau_values)\n",
    "\n",
    "plt.plot(x_positions, y_positions)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = {i: (x_positions[i], y_positions[i]) for i in range(nx_nodes)}\n",
    "\n",
    "node_colors = plt.cm.rainbow(np.linspace(0, 1, nx_nodes))\n",
    "\n",
    "plt.figure(figsize=(a, b))\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=300, alpha=0.9)\n",
    "\n",
    "curved_edges = [(u, v) for u, v in G.edges() if abs(u - v) > 1 and not (u == 0 and v == nx_nodes - 1)]  # Curved edges for longer jumps\n",
    "straight_edges = [(u, v) for u, v in G.edges() if abs(u - v) == 1 or (u == 0 and v == nx_nodes - 1)]  # Direct neighbors + periodic edges\n",
    "\n",
    "# Draw straight and curved edges separately\n",
    "nx.draw_networkx_edges(G, pos, edgelist=straight_edges, edge_color=\"gray\", width=1.5, alpha=0.7)\n",
    "nx.draw_networkx_edges(G, pos, edgelist=curved_edges, edge_color=\"gray\", width=1.5, alpha=0.7, style=\"dashed\")\n",
    "\n",
    "# Annotate nodes\n",
    "labels = {i: f\"N{i}\" for i in range(nx_nodes)}\n",
    "nx.draw_networkx_labels(G, pos, labels, font_size=9, font_weight=\"bold\")\n",
    "\n",
    "# Add edge labels (showing node connections)\n",
    "edge_labels = {(u, v): f\"{u}-{v}\" for u, v in edges}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=7, font_color=\"black\")\n",
    "\n",
    "plt.title(f\"Graph Structure for GNN\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"../assets/images/gnn_graph_connectivity.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.data as geom_data\n",
    "import torch_geometric.nn as geom_nn\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define parameters\n",
    "xa, nx, nt, v = 10, 25, 15, 0.6\n",
    "\n",
    "# Create grid and function data\n",
    "x_grid = np.linspace(0, xa, nx + 1)[:-1]\n",
    "z = np.zeros([nt, nx])\n",
    "for j in range(nt):\n",
    "    z[j, :] = np.sin((2 * np.pi / xa) * x_grid - v * j)\n",
    "\n",
    "# Create adjacency matrix\n",
    "p1 = np.sin(2 * np.pi * x_grid / xa)\n",
    "p2 = np.cos(2 * np.pi * x_grid / xa)\n",
    "p1m, p2m = np.tile(p1, (nx, 1)).T, np.tile(p2, (nx, 1)).T\n",
    "diff = np.sqrt((p1m - p1m.T) ** 2 + (p2m - p2m.T) ** 2)\n",
    "adjm = (diff < 0.5).astype(int)\n",
    "edge_index = torch.tensor(np.array(np.nonzero(adjm)), dtype=torch.long)\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, Y_train = z[:-1], z[1:]\n",
    "X_test, Y_test = z[:-1], z[1:]\n",
    "\n",
    "# Create feature tensors and data loader\n",
    "features_tmp2 = torch.tensor(np.arange(1, nx + 1) / nx, dtype=torch.float).unsqueeze(1)\n",
    "train_list, test_list = [], []\n",
    "for k in range(X_train.shape[0]):\n",
    "    features_k_tmp1 = torch.tensor(X_train[k, :], dtype=torch.float).unsqueeze(1)\n",
    "    features_k = torch.cat((features_k_tmp1, features_tmp2), dim=1)\n",
    "    labels_k = torch.tensor(Y_train[k, :], dtype=torch.float).unsqueeze(1)\n",
    "    data = geom_data.Data(x=features_k, y=labels_k, edge_index=edge_index)\n",
    "    train_list.append(data)\n",
    "\n",
    "for k in range(X_test.shape[0]):\n",
    "    features_k_tmp1 = torch.tensor(X_test[k, :], dtype=torch.float).unsqueeze(1)\n",
    "    features_k = torch.cat((features_k_tmp1, features_tmp2), dim=1)\n",
    "    labels_k = torch.tensor(Y_test[k, :], dtype=torch.float).unsqueeze(1)\n",
    "    data = geom_data.Data(x=features_k, y=labels_k, edge_index=edge_index)\n",
    "    test_list.append(data)\n",
    "\n",
    "# Create DataLoaders for training and testing\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader  # neuer Import\n",
    "\n",
    "train_loader = DataLoader(train_list, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_list, batch_size=1, shuffle=False)\n",
    "\n",
    "# Define the GNN model\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_feats_y):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = geom_nn.GCNConv(num_features, hidden_channels[0])\n",
    "        self.conv2 = geom_nn.GCNConv(hidden_channels[0], hidden_channels[1])\n",
    "        self.conv3 = geom_nn.GCNConv(hidden_channels[1], hidden_channels[2])\n",
    "        self.conv4 = geom_nn.GCNConv(hidden_channels[2], hidden_channels[3])\n",
    "        self.fc1 = nn.Linear(hidden_channels[3], hidden_channels[2])\n",
    "        self.fc2 = nn.Linear(hidden_channels[2], hidden_channels[0])\n",
    "        self.fc3 = nn.Linear(hidden_channels[0], num_feats_y)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv2(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv3(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv4(x, edge_index))\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Initialize model, optimizer, and criterion\n",
    "model = GNNModel(num_features=2, hidden_channels=[4 * nt, 4 * nt, 4 * nt, 4 * nt], num_feats_y=1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 1500\n",
    "train_mse, test_mse = [], []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    train_mse_tmp = []\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch.x, batch.edge_index)\n",
    "        loss = criterion(output, batch.y)\n",
    "        train_mse_tmp.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_mse.append(np.mean(train_mse_tmp))\n",
    "\n",
    "    model.eval()\n",
    "    test_mse_tmp = []\n",
    "    for batch in test_loader:\n",
    "        y_pred = model(batch.x, batch.edge_index)\n",
    "        test_loss = criterion(y_pred, batch.y)\n",
    "        test_mse_tmp.append(test_loss.item())\n",
    "    test_mse.append(np.mean(test_mse_tmp))\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Train Loss: {train_mse[epoch]}, Test Loss: {test_mse[epoch]}')\n",
    "\n",
    "# Plot training and test MSE\n",
    "plt.plot(np.arange(epochs), train_mse, '*', label='Train Loss')\n",
    "plt.plot(np.arange(epochs), test_mse, '*', label='Test Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Training and Test Loss\")\n",
    "plt.savefig(\"../assets/images/gnn_loss_curve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse_tmp = []\n",
    "\n",
    "# Counter for images\n",
    "ni = 1\n",
    "\n",
    "# Select a few test cases\n",
    "test_cases = np.random.choice(range(len(X_train) - 1), size=2, replace=False)\n",
    "\n",
    "for idx in test_cases:\n",
    "\n",
    "    # Get a batch from the selected test case\n",
    "    original_func = X_train[idx]\n",
    "    translated_func = X_train[idx + 1]\n",
    "    input_features = train_list[idx].x\n",
    "\n",
    "    # Predict with the model\n",
    "    with torch.no_grad():\n",
    "        predicted_func = model(input_features, train_list[idx].edge_index).numpy().flatten()\n",
    "\n",
    "    # Compute MSE for this test case\n",
    "    mse = np.mean((translated_func - predicted_func) **2)\n",
    "    test_mse_tmp.append(mse)\n",
    "\n",
    "    # Plot comparison for this test case (Original, Translated, and Predicted)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(original_func, label=\"Original Function\", linestyle='-', marker='o', color='blue')\n",
    "    plt.plot(translated_func, label=\"Translated Function\", linestyle='-', marker='x', color='green')\n",
    "    plt.plot(predicted_func, label=\"Predicted Translated Function\", linestyle='--', marker='s', color='red')\n",
    "    plt.title(f\"Function {idx} - MSE: {mse:.4f}\")\n",
    "    plt.legend()\n",
    "    plt.xlabel('Node index')\n",
    "    plt.ylabel('Function value')\n",
    "    plt.savefig(f\"../assets/images/gnn_test_{ni}.png\")\n",
    "    ni+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "## CNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_function_data(num_samples=5000, num_points=50, err=0.02):\n",
    "    X = []\n",
    "    y = []\n",
    "    functions = ['sine-cosine', 'gaussian', 'polynomial']\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        x = np.linspace(-1, 1, num_points)\n",
    "        func_type = np.random.choice(functions)\n",
    "\n",
    "        # Initialize a default y_values to prevent UnboundLocalError\n",
    "        y_values = np.zeros(num_points)\n",
    "        label = -1\n",
    "\n",
    "        if func_type == 'sine-cosine':\n",
    "            freq = np.random.uniform(1, 5)  \n",
    "            phase = np.random.uniform(0, 2 * np.pi)\n",
    "            amp = np.random.uniform(0.5, 2)\n",
    "            y_values = amp * np.sin(freq * np.pi * x + phase) + err * np.random.randn(num_points)\n",
    "            label = 0\n",
    "\n",
    "        elif func_type == 'gaussian':\n",
    "            mu = np.random.uniform(-0.5, 0.5)  \n",
    "            sigma = np.random.uniform(0.2, 0.5)  \n",
    "            amp = np.random.uniform(0.5, 2)\n",
    "            y_values = amp * np.exp(-((x - mu) ** 2) / (2 * sigma ** 2)) + err * np.random.randn(num_points)\n",
    "            label = 1\n",
    "\n",
    "        elif func_type == 'polynomial':\n",
    "            a = np.random.uniform(-2, 2)\n",
    "            b = np.random.uniform(-2, 2)\n",
    "            c = np.random.uniform(-3, 3)\n",
    "            d = np.random.uniform(-0.5, 0.5)\n",
    "            y_values = a * x**3 + b * x**2 + c * x + d + err * np.random.randn(num_points)\n",
    "            label = 2\n",
    "\n",
    "        X.append(y_values)\n",
    "        y.append(label)\n",
    "\n",
    "    X = np.array(X).reshape(-1, 1, num_points)  # Add channel dimension\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Generate a large training and test dataset with adjustable noise\n",
    "X_train, y_train = generate_function_data(num_samples=10000, err=0.05)  # Low noise in training\n",
    "X_test, y_test = generate_function_data(num_samples=2000, err=0.2)  # Higher noise in test set\n",
    "\n",
    "print(f\"Train Data Shape: {X_train.shape}, Train Labels Shape: {y_train.shape}\")\n",
    "print(f\"Test Data Shape: {X_test.shape}, Test Labels Shape: {y_test.shape}\")\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i, idx in enumerate(torch.randperm(len(X_train))[:6]):\n",
    "    plt.subplot(1, 6, i + 1)\n",
    "    plt.plot(X_train[idx][0].cpu().numpy())\n",
    "    plt.title(['sine-cosine', 'gaussian', 'polynomial'][y_train[idx].item()])\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_categories = 5\n",
    "\n",
    "class FunctionClassifierCNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=1,  out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 50, 128)\n",
    "        self.fc2 = nn.Linear(128, num_categories)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model\n",
    "model = FunctionClassifierCNN()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training ---\n",
    "\n",
    "# Setup\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print(\"device\", device)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "# Convert dataset into DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    list(zip(X_train, y_train)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "# Loss as a function of epochs\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        opt.zero_grad()\n",
    "        loss = criterion(model(batch_X), batch_y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item()\n",
    "    # Save epoch loss\n",
    "    loss_history.append(total_loss / len(train_loader))\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10,5))\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.savefig(\"../assets/images/cnn_training_loss.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    list(zip(X_test, y_test)),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        outputs = model(batch_X)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "accuracy =  100 * correct/total\n",
    "print(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "import random\n",
    "\n",
    "num_examples = 12\n",
    "\n",
    "X_new, y_new = generate_function_data(num_samples=num_examples)\n",
    "X_new = X_new.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_new)\n",
    "    _, predicted_labels = torch.max(predictions, 1)\n",
    "\n",
    "func_names = ['Sine-Cosine', 'Gaussian', 'Polynomial']\n",
    "\n",
    "# Plot the results\n",
    "rows = num_examples // 4  # Show 4 per row\n",
    "plt.figure(figsize=(12, 3 * rows))\n",
    "\n",
    "for i in range(num_examples):\n",
    "    correct = predicted_labels[i] == y_new[i]  # Check if prediction is correct\n",
    "    color = 'blue' if correct else 'red'  # Blue for correct, red for incorrect\n",
    "\n",
    "    plt.subplot(rows, 4, i + 1)\n",
    "    plt.plot(np.linspace(-1, 1, 50), X_new[i].cpu().numpy().squeeze(), color=color, label=f\"Pred: {func_names[predicted_labels[i]]}\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"True: {func_names[y_new[i]]}\", color=color)  # Color title for extra clarity\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../assets/images/cnn_test_predictions.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "## LSTM Sensor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate normal sine wave data with random phase shift\n",
    "def generate_sensor_data(num_samples=100, seq_length=50, anomaly_ratio=0.1):\n",
    "    x = []\n",
    "    labels = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        phase_shift = np.random.uniform(0, 2*np.pi)\n",
    "        time_series = np.sin(np.linspace(0, 2*np.pi, seq_length) + phase_shift) + 0.1 * np.random.rand(seq_length)\n",
    "        label = 0 # Normal\n",
    "\n",
    "        # Inject anomalies\n",
    "        if np.random.rand() < anomaly_ratio:\n",
    "            # Add large spikes\n",
    "            time_series += np.random.uniform(-2, 2, size=seq_length)\n",
    "            label = 1\n",
    "\n",
    "        x.append(time_series)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(x), np.array(labels)\n",
    "\n",
    "# Training/test data\n",
    "num_samples = 2000\n",
    "train_frac = 0.8\n",
    "bndry = math.floor(0.8*2000)\n",
    "X, y = generate_sensor_data(num_samples=num_samples)\n",
    "X_train, X_test = torch.tensor(X[:bndry], dtype=torch.float32), torch.tensor(X[bndry:], dtype=torch.float32)\n",
    "y_train, y_test = y[:bndry], y[bndry:]\n",
    "\n",
    "# Reshape for LSTM input\n",
    "X_train = X_train.unsqueeze(-1)\n",
    "X_test = X_test.unsqueeze(-1)\n",
    "\n",
    "print(f\"Train Data Shape: {X_train.shape}, Test Data Shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_indices = np.where(y_train == 0)[0][:3]\n",
    "anomaly_indices = np.where(y_train == 1)[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot normal sequences\n",
    "for i, idx in enumerate(normal_indices):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.plot(X_train[idx].squeeze().cpu().numpy(), label=\"Normal\", color=\"blue\")\n",
    "    plt.title(\"Normal Sensor Data\")\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "\n",
    "# Plot anomalous sequences\n",
    "for i, idx in enumerate(anomaly_indices):\n",
    "    plt.subplot(2, 3, i + 4)\n",
    "    plt.plot(X_train[idx].squeeze().cpu().numpy(), label=\"Anomaly\", color=\"red\")\n",
    "    plt.title(\"Anomalous Sensor Data\")\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../assets/images/lstm_sensor_data_samples.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=32, num_layers=2, seq_length=50):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layers\n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.decoder = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Final layer to reconstruct output\n",
    "        self.output_layer = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Encode input\n",
    "        _, (hidden, cell) = self.encoder(x)\n",
    "\n",
    "        # Initialise decoder input as zeros\n",
    "        decoder_input = torch.zeros(batch_size, self.seq_length, 1).to(x.device)\n",
    "\n",
    "        # Decode using last hidden state from encoder\n",
    "        decoder_output, _ = self.decoder(decoder_input, (hidden, cell))\n",
    "\n",
    "        x_reconstructed = self.output_layer(decoder_output)\n",
    "\n",
    "        return x_reconstructed    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model with correct sequence length\n",
    "model = LSTMAutoencoder(seq_length=50).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.010)\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(X_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Track loss history\n",
    "loss_history = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        opt.zero_grad()\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    loss_history.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history, label=\"Loss\")\n",
    "plt.xlabel(\"Epochs\"), plt.ylabel(\"Loss\"), plt.title(\"LSTM Training Loss\")\n",
    "plt.legend(), plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute reconstruction error on test data\n",
    "model.eval()\n",
    "\n",
    "X_test = X_test.to(device)\n",
    "with torch.no_grad():\n",
    "    X_reconstructed = model(X_test)\n",
    "\n",
    "reconstruction_errors = torch.mean((X_test - X_reconstructed)**2, dim=(1, 2)).cpu().numpy()\n",
    "\n",
    "# Set anomaly threshold\n",
    "threshold = np.percentile(reconstruction_errors, 95)\n",
    "y_pred = (reconstruction_errors > threshold).astype(int)\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test) * 100\n",
    "print(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "\n",
    "# Plot normal example\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(X_test[0].cpu().numpy(), label=\"Original\")\n",
    "plt.plot(X_reconstructed[0].cpu().numpy(), label=\"Reconstructed\", linestyle=\"dashed\")\n",
    "plt.title(\"Normal Sequence\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot anomaly example\n",
    "anomaly_idx = np.argmax(reconstruction_errors)  # Most anomalous sample\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(X_test[anomaly_idx].cpu().numpy(), label=\"Original\")\n",
    "plt.plot(X_reconstructed[anomaly_idx].cpu().numpy(), label=\"Reconstructed\", linestyle=\"dashed\", color=\"red\")\n",
    "plt.title(\"Anomalous Sequence\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../assets/images/lstm_anomaly_detection.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Select 12 random test samples\n",
    "num_samples = 12\n",
    "indices = np.random.choice(len(X_test), num_samples, replace=False)\n",
    "\n",
    "# Compute reconstruction errors\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_reconstructed = model(X_test.to(device))\n",
    "\n",
    "reconstruction_errors = torch.mean((X_test - X_reconstructed) ** 2, dim=(1, 2)).cpu().numpy()\n",
    "\n",
    "# Detect anomalies based on threshold\n",
    "threshold = np.percentile(reconstruction_errors, 90)\n",
    "y_pred = (reconstruction_errors > threshold).astype(int)  # 1 = Anomaly, 0 = Normal\n",
    "\n",
    "# Plot the selected samples\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, idx in enumerate(indices):\n",
    "    color = 'red' if y_pred[idx] == 1 else 'blue'\n",
    "    \n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    plt.plot(X_test[idx].cpu().numpy(), color=color, label=\"Original\")\n",
    "    plt.plot(X_reconstructed[idx].cpu().numpy(), linestyle=\"dashed\", color=\"black\", label=\"Reconstructed\")\n",
    "    plt.title(f\"{'Anomaly' if y_pred[idx] == 1 else 'Normal'}\", color=color)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.legend(fontsize=8, loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../assets/images/lstm_anomaly_detection_samples.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139",
   "metadata": {},
   "source": [
    "# Lecture 6 - LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "141",
   "metadata": {},
   "source": [
    "# Lecture 7 - RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "143",
   "metadata": {},
   "source": [
    "# Lecture 8 - Multimodal LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "145",
   "metadata": {},
   "source": [
    "# Lecture 9 - Diffusion and Graph Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 1D distribution p(x)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Target distribution: mixture of Gaussians\n",
    "def sample_target(n):\n",
    "    comp = torch.randint(0, 3, (n,))\n",
    "    means = torch.tensor([-2.0, 0.5, 2.5])\n",
    "    stds  = torch.tensor([0.3, 0.2, 0.4])\n",
    "    x = torch.randn(n) * stds[comp] + means[comp]\n",
    "    return x.unsqueeze(1)\n",
    "\n",
    "# Draw reference samples\n",
    "x_ref = sample_target(20_000).numpy()\n",
    "\n",
    "plt.hist(x_ref, bins=200, density=True)\n",
    "plt.title(\"Target distribution p(x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NN - a straighforward FFNN / MLP\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "# (Used as a loss function in the standard way in the training loop;\n",
    "# but comparing the *distribution* of the predicted and true samples,\n",
    "# as opposed to pointwise comparison of paired x, \\hat{y}\n",
    "\n",
    "# NB \"sliced\" Wasserstein in 1D is just Wasserstein\n",
    "# Slicing relevant for higher dimensions where true Wasserstein expensive,\n",
    "# so project onto random 1D slices\n",
    "\n",
    "def sliced_wasserstein_1d(x_fake, x_real):\n",
    "    # Earth mover's distance in 1D is just: sort both distributions\n",
    "    # and compare element-wise\n",
    "    x_fake_sorted, _ = torch.sort(x_fake.view(-1))\n",
    "    x_real_sorted, _ = torch.sort(x_real.view(-1))\n",
    "    return torch.mean((x_fake_sorted - x_real_sorted) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "# Instantiate the network\n",
    "G = Generator()\n",
    "\n",
    "# Define optimiser\n",
    "optimizer = torch.optim.Adam(G.parameters(), lr=1e-3)\n",
    "\n",
    "n_samples = 4096\n",
    "for epoch in range(3001):\n",
    "\n",
    "    z = torch.randn(n_samples, 1)\n",
    "\n",
    "    # Generate fake / generated data for this epoch\n",
    "    x_fake = G(z)\n",
    "\n",
    "    # Real data drawn from the known true distribution, for this epoch\n",
    "    x_real = sample_target(n_samples)\n",
    "\n",
    "    loss = sliced_wasserstein_1d(x_fake, x_real)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 300 == 0:\n",
    "        print(f\"Epoch {epoch:4d} | loss = {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(50_000, 1)\n",
    "    x_gen = G(z).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(x_ref, bins=200, density=True, alpha=0.5, label=\"Target\")\n",
    "plt.hist(x_gen, bins=200, density=True, alpha=0.5, label=\"Generated\")\n",
    "plt.legend()\n",
    "plt.title(\"True distribution vs learned sampler\")\n",
    "plt.savefig(\"../assets/images/1d_distribution_sampling.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "154",
   "metadata": {},
   "source": [
    "# MLOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/seppe-intelliprove/face-detection-onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fdlite import FaceDetection, FaceDetectionModel\n",
    "from fdlite.render import Colors, detections_to_render_data, render_to_image\n",
    "import PIL\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(image: PIL.Image):\n",
    "    detect_faces = FaceDetection(model_type=FaceDetectionModel.BACK_CAMERA)\n",
    "    faces = detect_faces(image)\n",
    "    print(f\"Found {len(faces)} faces\")\n",
    "    return faces\n",
    "\n",
    "\n",
    "def mark_faces(image_filename):\n",
    "    \"\"\"Mark all faces recognized in the image\"\"\"\n",
    "    image = PIL.Image.open(image_filename)\n",
    "\n",
    "    faces = detect_faces(image)\n",
    "\n",
    "    # Draw faces\n",
    "    render_data = detections_to_render_data(\n",
    "        faces, bounds_color=Colors.GREEN, line_width=3\n",
    "    )\n",
    "    render_to_image(render_data, image)\n",
    "\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://upload.wikimedia.org/wikipedia/commons/3/3d/Apollo_11_Crew.jpg\n",
    "mark_faces(\"Apollo_11_Crew.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -A \"Mozilla/5.0\" \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/07/Isabella_L%C3%B6vin_signing_climate_law_referral.jpg/1024px-Isabella_L%C3%B6vin_signing_climate_law_referral.jpg\" -o IL.jpg\n",
    "mark_faces(\"IL.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -A \"Mozilla/5.0\" \"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/20180610_FIFA_Friendly_Match_Austria_vs._Brazil_Miranda_850_0051.jpg/1024px-20180610_FIFA_Friendly_Match_Austria_vs._Brazil_Miranda_850_0051.jpg\" -o FIFA.jpg\n",
    "mark_faces(\"FIFA.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "162",
   "metadata": {},
   "source": [
    "# Model emulator; AIFS and AICON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "164",
   "metadata": {},
   "source": [
    "# AI Data Assimilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165",
   "metadata": {},
   "source": [
    "## Modulated sine background with 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 256\n",
    "x_grid = np.linspace(0.0, 1.0, n, endpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# \"True\" state: modulated sine\n",
    "#   y(x) = A(x) * sin(2π k x + phase) + trend\n",
    "# ----------------------------\n",
    "k = 3.0\n",
    "phase = 0.4\n",
    "A0 = 1.0\n",
    "A1 = 0.35\n",
    "A_mod_k = 1.0  # modulation wavenumber\n",
    "\n",
    "A = A0 + A1 * np.sin(2*np.pi*A_mod_k * x_grid + 0.7)\n",
    "trend = 0.15 * (x_grid - 0.5)\n",
    "x_true = A * np.sin(2*np.pi*k * x_grid + phase) + trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_grid, x_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Background xb: biased + smoothed + small noise\n",
    "# ----------------------------\n",
    "bias = 0.10\n",
    "shift = 4  # grid points, periodic shift\n",
    "x_shifted = np.roll(x_true, shift)\n",
    "\n",
    "# simple smoothing via convolution (periodic padding)\n",
    "sigma_pts = 2.0\n",
    "radius = int(np.ceil(4 * sigma_pts))\n",
    "t = np.arange(-radius, radius + 1)\n",
    "ker = np.exp(-(t**2) / (2 * sigma_pts**2))\n",
    "ker /= ker.sum()\n",
    "\n",
    "x_pad = np.r_[x_shifted[-radius:], x_shifted, x_shifted[:radius]]\n",
    "x_smooth = np.convolve(x_pad, ker, mode=\"same\")[radius:-radius]\n",
    "\n",
    "xb = x_smooth + bias + 0.03 * rng.standard_normal(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_grid, x_true)\n",
    "plt.plot(x_grid, xb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
